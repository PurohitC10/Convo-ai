{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/chirayupurohit/chirayu-101903011?scriptVersionId=90261994\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown","outputs":[],"execution_count":0},{"cell_type":"code","source":"import cudf as pd\nimport cupy as np","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-03-16T09:53:07.939755Z","iopub.execute_input":"2022-03-16T09:53:07.940075Z","iopub.status.idle":"2022-03-16T09:53:11.754704Z","shell.execute_reply.started":"2022-03-16T09:53:07.939988Z","shell.execute_reply":"2022-03-16T09:53:11.75393Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('../input/nlp-getting-started/train.csv')\ntest = pd.read_csv('../input/nlp-getting-started/test.csv')","metadata":{"execution":{"iopub.status.busy":"2022-03-16T09:53:13.548385Z","iopub.execute_input":"2022-03-16T09:53:13.548794Z","iopub.status.idle":"2022-03-16T09:53:15.704932Z","shell.execute_reply.started":"2022-03-16T09:53:13.54876Z","shell.execute_reply":"2022-03-16T09:53:15.704177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.info()","metadata":{"execution":{"iopub.status.busy":"2022-03-16T09:53:17.144001Z","iopub.execute_input":"2022-03-16T09:53:17.144814Z","iopub.status.idle":"2022-03-16T09:53:17.174095Z","shell.execute_reply.started":"2022-03-16T09:53:17.144763Z","shell.execute_reply":"2022-03-16T09:53:17.173367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.info()","metadata":{"execution":{"iopub.status.busy":"2022-03-16T09:53:18.288734Z","iopub.execute_input":"2022-03-16T09:53:18.289189Z","iopub.status.idle":"2022-03-16T09:53:18.301665Z","shell.execute_reply.started":"2022-03-16T09:53:18.289151Z","shell.execute_reply":"2022-03-16T09:53:18.300771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head(20)","metadata":{"execution":{"iopub.status.busy":"2022-03-16T09:53:22.241793Z","iopub.execute_input":"2022-03-16T09:53:22.24234Z","iopub.status.idle":"2022-03-16T09:53:22.28381Z","shell.execute_reply.started":"2022-03-16T09:53:22.242302Z","shell.execute_reply":"2022-03-16T09:53:22.283089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.head(20)","metadata":{"execution":{"iopub.status.busy":"2022-03-16T09:53:23.538197Z","iopub.execute_input":"2022-03-16T09:53:23.538733Z","iopub.status.idle":"2022-03-16T09:53:23.564751Z","shell.execute_reply.started":"2022-03-16T09:53:23.538699Z","shell.execute_reply":"2022-03-16T09:53:23.564086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.keyword.unique()","metadata":{"execution":{"iopub.status.busy":"2022-03-16T09:53:24.506962Z","iopub.execute_input":"2022-03-16T09:53:24.507588Z","iopub.status.idle":"2022-03-16T09:53:24.532174Z","shell.execute_reply.started":"2022-03-16T09:53:24.507548Z","shell.execute_reply":"2022-03-16T09:53:24.531355Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.keyword.unique()","metadata":{"execution":{"iopub.status.busy":"2022-03-16T09:53:25.351808Z","iopub.execute_input":"2022-03-16T09:53:25.352096Z","iopub.status.idle":"2022-03-16T09:53:25.368379Z","shell.execute_reply.started":"2022-03-16T09:53:25.352065Z","shell.execute_reply":"2022-03-16T09:53:25.367662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\nimport nltk\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\nfrom string import punctuation","metadata":{"execution":{"iopub.status.busy":"2022-03-16T09:57:31.364988Z","iopub.execute_input":"2022-03-16T09:57:31.365301Z","iopub.status.idle":"2022-03-16T09:57:31.369831Z","shell.execute_reply.started":"2022-03-16T09:57:31.365266Z","shell.execute_reply":"2022-03-16T09:57:31.369037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Removing URLS\ndef url_clean(text):\n    url = re.compile(r'https?://\\S+|www\\.\\S+')\n    return url.sub(r'',text)","metadata":{"execution":{"iopub.status.busy":"2022-03-16T09:57:52.553363Z","iopub.execute_input":"2022-03-16T09:57:52.554126Z","iopub.status.idle":"2022-03-16T09:57:52.558911Z","shell.execute_reply.started":"2022-03-16T09:57:52.55409Z","shell.execute_reply":"2022-03-16T09:57:52.55809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Removing HTML tags\ndef html_clean(text):\n    html = re.compile(r'<.*?>')\n    return html.sub(r'',text)","metadata":{"execution":{"iopub.status.busy":"2022-03-16T09:58:19.884094Z","iopub.execute_input":"2022-03-16T09:58:19.884773Z","iopub.status.idle":"2022-03-16T09:58:19.890835Z","shell.execute_reply.started":"2022-03-16T09:58:19.884739Z","shell.execute_reply":"2022-03-16T09:58:19.890108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)","metadata":{"execution":{"iopub.status.busy":"2022-03-16T09:58:31.885205Z","iopub.execute_input":"2022-03-16T09:58:31.886039Z","iopub.status.idle":"2022-03-16T09:58:31.893162Z","shell.execute_reply.started":"2022-03-16T09:58:31.885994Z","shell.execute_reply":"2022-03-16T09:58:31.8923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# lower case\ndef to_lower(text):\n    return text.lower()","metadata":{"execution":{"iopub.status.busy":"2022-03-16T09:58:42.265066Z","iopub.execute_input":"2022-03-16T09:58:42.265793Z","iopub.status.idle":"2022-03-16T09:58:42.272156Z","shell.execute_reply.started":"2022-03-16T09:58:42.26575Z","shell.execute_reply":"2022-03-16T09:58:42.271402Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lemmatization\nfrom nltk.stem import WordNetLemmatizer\nwordnet_lemmatizer = WordNetLemmatizer()\ndef lemmatize(text):\n    lemmatized_word = [wordnet_lemmatizer.lemmatize(word) for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n    return \" \".join(lemmatized_word)","metadata":{"execution":{"iopub.status.busy":"2022-03-16T09:59:54.399113Z","iopub.execute_input":"2022-03-16T09:59:54.399983Z","iopub.status.idle":"2022-03-16T09:59:54.405111Z","shell.execute_reply.started":"2022-03-16T09:59:54.399942Z","shell.execute_reply":"2022-03-16T09:59:54.404124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# remove stopwords\ndef remove_stopwords(sentence):\n    stop_words = stopwords.words('english')\n    return ' '.join([x for x in nltk.word_tokenize(sentence) if not x in stop_words])","metadata":{"execution":{"iopub.status.busy":"2022-03-16T10:00:09.755071Z","iopub.execute_input":"2022-03-16T10:00:09.755336Z","iopub.status.idle":"2022-03-16T10:00:09.763389Z","shell.execute_reply.started":"2022-03-16T10:00:09.755308Z","shell.execute_reply":"2022-03-16T10:00:09.762607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# remove whitespace\ndef to_strip(text):\n    return \" \".join(text.split())","metadata":{"execution":{"iopub.status.busy":"2022-03-16T10:00:19.456807Z","iopub.execute_input":"2022-03-16T10:00:19.457063Z","iopub.status.idle":"2022-03-16T10:00:19.462813Z","shell.execute_reply.started":"2022-03-16T10:00:19.457033Z","shell.execute_reply":"2022-03-16T10:00:19.461931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# remove Punctuation\nfrom string import punctuation\ndef remove_punct(text):\n    return ''.join(x for x in text if x not in punctuation)","metadata":{"execution":{"iopub.status.busy":"2022-03-16T10:00:36.618875Z","iopub.execute_input":"2022-03-16T10:00:36.61913Z","iopub.status.idle":"2022-03-16T10:00:36.623888Z","shell.execute_reply.started":"2022-03-16T10:00:36.619103Z","shell.execute_reply":"2022-03-16T10:00:36.622811Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Contraction\ncontractions_dict = {     \n\"ain't\": \"am not\",\n\"aren't\": \"are not\",\n\"can't\": \"cannot\",\n\"can't've\": \"cannot have\",\n\"'cause\": \"because\",\n\"could've\": \"could have\",\n\"couldn't\": \"could not\",\n\"couldn't've\": \"could not have\",\n\"didn't\": \"did not\",\n\"doesn't\": \"does not\",\n\"don't\": \"do not\",\n\"hadn't\": \"had not\",\n\"hadn't've\": \"had not have\",\n\"hasn't\": \"has not\",\n\"haven't\": \"have not\",\n\"he'd\": \"he had\",\n\"he'd've\": \"he would have\",\n\"he'll\": \"he will\",\n\"he'll've\": \"he will have\",\n\"he's\": \"he is\",\n\"how'd\": \"how did\",\n    \"how'd'y\": \"how do you\",\n\"how'll\": \"how will\",\n\"how's\": \"how is\",\n\"I'd\": \"I had\",\n\"I'd've\": \"I would have\",\n\"I'll\": \"I will\",\n\"I'll've\": \"I will have\",\n\"I'm\": \"I am\",\n\"I've\": \"I have\",\n\"isn't\": \"is not\",\n\"it'd\": \"it had\",\n\"it'd've\": \"it would have\",\n\"it'll\": \"it will\",\n\"it'll've\": \"iit will have\",\n\"it's\": \"it is\",\n\"let's\": \"let us\",\n\"ma'am\": \"madam\",\n\"mayn't\": \"may not\",\n\"might've\": \"might have\",\n\"mightn't\": \"might not\",\n\"mightn't've\": \"might not have\",\n\"must've\": \"must have\",\n\"mustn't\": \"must not\",\n    \"mustn't've\": \"must not have\",\n\"needn't\": \"need not\",\n\"needn't've\": \"need not have\",\n\"o'clock\": \"of the clock\",\n\"oughtn't\": \"ought not\",\n\"oughtn't've\": \"ought not have\",\n\"shan't\": \"shall not\",\n\"sha'n't\": \"shall not\",\n\"shan't've\": \"shall not have\",\n\"she'd\": \"she had\",\n\"she'd've\": \"she would have\",\n\"she'll\": \"she will\",\n\"she'll've\": \"she will have\",\n\"she's\": \"she is\",\n\"should've\": \"should have\",\n\"shouldn't\": \"should not\",\n\"shouldn't've\": \"should not have\",\n\"so've\": \"so have\",\n\"so's\": \"so is\",\n\"that'd\": \"that had\",\n\"that'd've\": \"that would have\",\n\"that's\": \"that is\",\n\"there'd\": \"there had\",\n\"there'd've\": \"there would have\",\n    \"there's\": \"there is\",\n\"they'd\": \"they had\",\n\"they'd've\": \"they would have\",\n\"they'll\": \"they will\",\n\"they'll've\": \"they will have\",\n\"they're\": \"they are\",\n\"they've\": \"they have\",\n\"to've\": \"to have\",\n\"wasn't\": \"was not\",\n\"we'd\": \"we had\",\n\"we'd've\": \"we would have\",\n\"we'll\": \"we will\",\n\"we'll've\": \"we will have\",\n\"we're\": \"we are\",\n\"we've\": \"we have\",\n\"weren't\": \"were not\",\n\"what'll\": \"what will\",\n\"what'll've\": \"what will have\",\n\"what're\": \"what are\",\n\"what's\": \"what is\",\n\"what've\": \"what have\",\n\"when's\": \"when is\",\n\"when've\": \"when have\",\n\"where'd\": \"where did\",\n\"where's\": \"where is\",\n    \"where've\": \"where have\",\n\"who'll\": \"who will\",\n\"who'll've\": \"who will have\",\n\"who's\": \"who is\",\n\"who've\": \"who have\",\n\"why's\": \"why is\",\n\"why've\": \"why have\",\n\"will've\": \"will have\",\n\"won't\": \"will not\",\n\"won't've\": \"will not have\",\n\"would've\": \"would have\",\n\"wouldn't\": \"would not\",\n\"wouldn't've\": \"would not have\",\n\"y'all\": \"you all\",\n\"y'all'd\": \"you all would\",\n\"y'all'd've\": \"you all would have\",\n\"y'all're\": \"you all are\",\n\"y'all've\": \"you all have\",\n\"you'd\": \"you had\",\n\"you'd've\": \"you would have\",\n\"you'll\": \"you will\",\n\"you'll've\": \"you will have\",\n\"you're\": \"you are\",\n\"you've\": \"you have\"\n}\ndef expand_contractions(text, contractions_dict):\n    contractions_pattern = re.compile('({})'.format('|'.join(contractions_dict.keys())),\n                                      flags=re.IGNORECASE | re.DOTALL)\n\n    def expand_match(contraction):\n        match = contraction.group(0)\n        first_char = match[0]\n        expanded_contraction = contractions_dict.get(match) \\\n            if contractions_dict.get(match) \\\n            else contractions_dict.get(match.lower())\n        expanded_contraction = expanded_contraction\n        return expanded_contraction\n\n    expanded_text = contractions_pattern.sub(expand_match, text)\n    expanded_text = re.sub(\"'\", \"\", expanded_text)\n    return expanded_text\n\ndef main_contraction(text):\n    text = expand_contractions(text, contractions_dict)\n    return text","metadata":{"execution":{"iopub.status.busy":"2022-03-16T10:02:15.688295Z","iopub.execute_input":"2022-03-16T10:02:15.688643Z","iopub.status.idle":"2022-03-16T10:02:15.706938Z","shell.execute_reply.started":"2022-03-16T10:02:15.688609Z","shell.execute_reply":"2022-03-16T10:02:15.706129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['text'] = train['text'].to_pandas().apply(lambda x: url_clean(x))\ntrain['text']= train['text'].to_pandas().apply(lambda x : html_clean(x))\ntrain['text']= train['text'].to_pandas().apply(lambda x : remove_emoji(x))\ntrain['text']= train['text'].to_pandas().apply(lambda x :to_lower(x))\ntrain['text']= train['text'].to_pandas().apply(lambda x : main_contraction(x))\ntrain['text']= train['text'].to_pandas().apply(lambda x : remove_punct(x))\ntrain['text']= train['text'].to_pandas().apply(lambda x : to_strip(x))\ntrain['text']= train['text'].to_pandas().apply(lambda x : remove_stopwords(x))\ntrain['text'] = train['text'].to_pandas().apply(lemmatize)\ntrain['text'].head(5)","metadata":{"execution":{"iopub.status.busy":"2022-03-16T10:16:40.692371Z","iopub.execute_input":"2022-03-16T10:16:40.69263Z","iopub.status.idle":"2022-03-16T10:16:46.243161Z","shell.execute_reply.started":"2022-03-16T10:16:40.692602Z","shell.execute_reply":"2022-03-16T10:16:46.242251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test['text']= test['text'].to_pandas().apply(lambda x : url_clean(x))\ntest['text']= test['text'].to_pandas().apply(lambda x : html_clean(x))\ntest['text']= test['text'].to_pandas().apply(lambda x : remove_emoji(x))\ntest['text']= test['text'].to_pandas().apply(lambda x :to_lower(x))\ntest['text']= test['text'].to_pandas().apply(lambda x : main_contraction(x))\ntest['text']= test['text'].to_pandas().apply(lambda x : remove_punct(x))\ntest['text']= test['text'].to_pandas().apply(lambda x : to_strip(x))\ntest['text']= test['text'].to_pandas().apply(lambda x : remove_stopwords(x))\ntest['text'] =test['text'].to_pandas().apply(lemmatize)","metadata":{"execution":{"iopub.status.busy":"2022-03-16T10:15:14.031048Z","iopub.execute_input":"2022-03-16T10:15:14.031336Z","iopub.status.idle":"2022-03-16T10:15:16.725252Z","shell.execute_reply.started":"2022-03-16T10:15:14.031305Z","shell.execute_reply":"2022-03-16T10:15:16.72449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Word Frequency\nfrom collections import Counter\ndef count_df(df):\n    '''Return the count of values in dataset'''\n    counter = Counter()\n    for text in df.to_pandas():\n        for word in text.split():\n            counter[word] +=1\n#     print(type(counter))\n    return counter\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nmost_common_words = count_df(train[\"text\"]).most_common(15) \nmost_common_words, corresponding_values = zip(*most_common_words)\nplt.bar(most_common_words, corresponding_values)\nplt.title(\"10 most common words in the dataset\")\nplt.xlabel(\"words\")\nplt.ylabel(\"word count\")\nplt.xticks(rotation= 30)","metadata":{"execution":{"iopub.status.busy":"2022-03-16T10:28:37.09846Z","iopub.execute_input":"2022-03-16T10:28:37.099095Z","iopub.status.idle":"2022-03-16T10:28:37.388769Z","shell.execute_reply.started":"2022-03-16T10:28:37.099059Z","shell.execute_reply":"2022-03-16T10:28:37.388082Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"non_disastrous_words = count_df(train.loc[train['target'] == 0][\"text\"]).most_common(15)\nmost_non_disastrous_words, most_non_disastrous_words_values = zip(*non_disastrous_words)\nplt.bar(most_non_disastrous_words, most_non_disastrous_words_values, color='y')\nplt.title(\"non-disasterous tweets top word frequencies\")\nplt.xlabel(\"words\")\nplt.ylabel(\"word count\")\nplt.xticks(rotation= 30)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-16T10:30:09.205032Z","iopub.execute_input":"2022-03-16T10:30:09.205292Z","iopub.status.idle":"2022-03-16T10:30:09.457928Z","shell.execute_reply.started":"2022-03-16T10:30:09.205264Z","shell.execute_reply":"2022-03-16T10:30:09.457109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"disastrous_words = count_df(train.loc[train['target'] == 1][\"text\"]).most_common(15)\nmost_disastrous_words, most_disastrous_words_values = zip(*disastrous_words)\nplt.bar(most_disastrous_words, most_disastrous_words_values, color='b')\nplt.title(\"disasterous tweets top word frequencies\")\nplt.xlabel(\"words\")\nplt.ylabel(\"word count\")\nplt.xticks(rotation= 30)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-16T10:29:50.192876Z","iopub.execute_input":"2022-03-16T10:29:50.193155Z","iopub.status.idle":"2022-03-16T10:29:50.471262Z","shell.execute_reply.started":"2022-03-16T10:29:50.193123Z","shell.execute_reply":"2022-03-16T10:29:50.470463Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from cuml.feature_extraction.text import CountVectorizer\nfrom cuml.model_selection import train_test_split\nfrom cuml.ensemble import RandomForestClassifier\nfrom cuml.linear_model import LogisticRegression\nfrom cuml.svm import SVC\nfrom cuml.neighbors import KNeighborsClassifier\n","metadata":{"execution":{"iopub.status.busy":"2022-03-16T10:41:13.145459Z","iopub.execute_input":"2022-03-16T10:41:13.1462Z","iopub.status.idle":"2022-03-16T10:41:13.152038Z","shell.execute_reply.started":"2022-03-16T10:41:13.146153Z","shell.execute_reply":"2022-03-16T10:41:13.151159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"count_vectorizer = CountVectorizer()\ntrain_vectors = count_vectorizer.fit_transform(train[\"text\"])\ntest_vectors = count_vectorizer.transform(test[\"text\"])\ntrain_vectors = train_vectors.toarray()\ntest_vectors = test_vectors.toarray()","metadata":{"execution":{"iopub.status.busy":"2022-03-16T10:46:21.050486Z","iopub.execute_input":"2022-03-16T10:46:21.050742Z","iopub.status.idle":"2022-03-16T10:46:31.655341Z","shell.execute_reply.started":"2022-03-16T10:46:21.050716Z","shell.execute_reply":"2022-03-16T10:46:31.654614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"knn = KNeighborsClassifier()\nlogreg = LogisticRegression()\nrf = RandomForestClassifier()\nsvm = SVC()\n\nmodels = [knn,logreg, rf,svm]\nmodel_names = ['KNN','Logistic Regression', 'Random Forest', 'SVM']\n\n","metadata":{"execution":{"iopub.status.busy":"2022-03-16T10:56:32.38321Z","iopub.execute_input":"2022-03-16T10:56:32.383933Z","iopub.status.idle":"2022-03-16T10:56:32.390752Z","shell.execute_reply.started":"2022-03-16T10:56:32.383898Z","shell.execute_reply":"2022-03-16T10:56:32.389916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_targets =  train[\"target\"].values\ntrain, val, train_labels, val_labels = train_test_split(train_vectors, train_targets, test_size= 0.1, stratify= train_targets, random_state= 42)\n                                                        ","metadata":{"execution":{"iopub.status.busy":"2022-03-16T10:54:26.540191Z","iopub.execute_input":"2022-03-16T10:54:26.540755Z","iopub.status.idle":"2022-03-16T10:54:35.041776Z","shell.execute_reply.started":"2022-03-16T10:54:26.540719Z","shell.execute_reply":"2022-03-16T10:54:35.041027Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report\nfor model, model_name in zip(models, model_names):\n    model_instance = model\n    model_instance.fit(train, train_labels) # Fitting models \n    print(f'For {model_name}:')\n    pred = model_instance.predict(val) # Predictions on validation set\n    print(classification_report(val_labels.get(), pred.get()))","metadata":{"execution":{"iopub.status.busy":"2022-03-16T11:03:21.897135Z","iopub.execute_input":"2022-03-16T11:03:21.897731Z","iopub.status.idle":"2022-03-16T11:03:33.852727Z","shell.execute_reply.started":"2022-03-16T11:03:21.897689Z","shell.execute_reply":"2022-03-16T11:03:33.851946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can clearly see that SVM and Logistic Regression are having maximum accuracy","metadata":{}},{"cell_type":"markdown","source":"Now we can import accuracy score and check the accurancy score individually to get to a better result","metadata":{}},{"cell_type":"code","source":"from cuml.metrics import accuracy_score\nlr = LogisticRegression()\nlr.fit(train_vectors, train_targets)\nprint('Accuracy of Logistic Regression: ')\nprint(np.round(accuracy_score(train_targets, lr.predict(train_vectors)) * 100, 2))","metadata":{"execution":{"iopub.status.busy":"2022-03-16T11:12:35.013585Z","iopub.execute_input":"2022-03-16T11:12:35.013861Z","iopub.status.idle":"2022-03-16T11:12:35.223763Z","shell.execute_reply.started":"2022-03-16T11:12:35.013831Z","shell.execute_reply":"2022-03-16T11:12:35.222917Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"svm = SVC()\nsvm.fit(train_vectors, train_targets)\nprint('Accuracy of SVM: ')\nprint(np.round(accuracy_score(train_targets, svm.predict(train_vectors)) * 100, 2))","metadata":{"execution":{"iopub.status.busy":"2022-03-16T11:13:34.412543Z","iopub.execute_input":"2022-03-16T11:13:34.412801Z","iopub.status.idle":"2022-03-16T11:13:36.729247Z","shell.execute_reply.started":"2022-03-16T11:13:34.412772Z","shell.execute_reply":"2022-03-16T11:13:36.728535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can now say that Logistic Regresion is giving better accuracy","metadata":{}},{"cell_type":"code","source":"predictions = lr.predict(test_vectors)\nsubmission_file = pd.read_csv(\"/kaggle/input/nlp-getting-started/sample_submission.csv\")\nsubmission_file['target'] = predictions\nsubmission_file.to_csv(\"submission.csv\", index= False)","metadata":{"execution":{"iopub.status.busy":"2022-03-16T11:16:51.044084Z","iopub.execute_input":"2022-03-16T11:16:51.04465Z","iopub.status.idle":"2022-03-16T11:16:51.063039Z","shell.execute_reply.started":"2022-03-16T11:16:51.044609Z","shell.execute_reply":"2022-03-16T11:16:51.062283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}